{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vietnamese-Q&A-wiki.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMqduU3Ylex6C+AjmkihHcW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koyomin9zx/UITQA-Vietnamese-Question-Answering/blob/master/Vietnamese_Q%26A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwGlBWUyHxBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Requirements\n",
        "!pip install --upgrade google-api-python-client\n",
        "!pip install nltk\n",
        "!pip install BeautifulSoup\n",
        "!pip install underthesea\n",
        "!pip install sklearn\n",
        "!pip install plotly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byNYORPPIX9Z",
        "colab_type": "code",
        "outputId": "c4d2ecda-3f68-4e51-e266-0d88c00574ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import pickle\n",
        "from googleapiclient.discovery import build\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import timeit\n",
        "import time\n",
        "from multiprocessing import Pool\n",
        "import string\n",
        "import numpy as np\n",
        "from difflib import SequenceMatcher\n",
        "from nltk import sent_tokenize, download\n",
        "from underthesea import word_tokenize\n",
        "from underthesea import ner\n",
        "from collections import defaultdict\n",
        "\n",
        "download('punkt')\n",
        "Seach_api_key =\"AIzaSyBy-PVoHZdYRDU70gsLD-ALy5JabcZUICk\"\n",
        "Custom_Search_Engine_ID =\"013964321510468908374:fcs9cr0koid\"\n",
        "\n",
        "stopwords = set(open('stopwords.txt').read().split('\\n')[:-1]) #set stopword\n",
        "puct_set = set([c for c in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~']) #set puctua\n",
        "\n",
        "def tokenize(text):\n",
        "    sents = sent_tokenize(text)\n",
        "    sents = [word_tokenize(s,format = 'text') for s in sents]\n",
        "    return sents\n",
        "\n",
        "def get_entities(seq):\n",
        "    i = 0\n",
        "    chunks = []\n",
        "    seq = seq + ['O']  # add sentinel\n",
        "    types = [tag.split('-')[-1] for tag in seq]\n",
        "    while i < len(seq):\n",
        "        if seq[i].startswith('B'):\n",
        "            for j in range(i+1, len(seq)):\n",
        "                if seq[j].startswith('I') and types[j] == types[i]:\n",
        "                    continue\n",
        "                break\n",
        "            chunks.append((types[i], i, j))\n",
        "            i = j\n",
        "        else:\n",
        "            i += 1\n",
        "    return chunks\n",
        "\n",
        "def _get_chunks(words, tags):\n",
        "    chunks = get_entities(tags)\n",
        "    res = defaultdict(list)\n",
        "    for chunk_type, chunk_start, chunk_end in chunks:\n",
        "        res[chunk_type].append(' '.join(words[chunk_start: chunk_end]))\n",
        "    return res\n",
        "\n",
        "def ner_extraction(text):\n",
        "    res = ner(text)\n",
        "    words = [r[0] for r in res]\n",
        "    tags = [t[3] for t in res]\n",
        "    \n",
        "    return _get_chunks(words,tags)\n",
        "\n",
        "def generateBigram(words):\n",
        "    bigrams = [words[i] + '_' + words[i+1] for i in range(0,len(words) - 1)]\n",
        "    return bigrams\n",
        "\n",
        "def noiseSent(sent):\n",
        "    if len(sent.split()) <= 3 or len(sent.split()) > 100:\n",
        "        return True\n",
        "    \n",
        "    if len(sent) <= 30:\n",
        "        return True\n",
        "    \n",
        "    if all(ord(c) < 128 for c in sent):\n",
        "        return True\n",
        "    \n",
        "    if not any(c.isalpha() for c in sent):\n",
        "        return True\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sI6LEtLKeI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Passage:\n",
        "    def __init__(self,string,rank,num_key):\n",
        "        self.sent = string            #sentences\n",
        "        self.ner = []                 #named entities\n",
        "        self.num_key = num_key        #number of match keywords\n",
        "        self.len_long_seq = 0         #length of longest exact sequence of question keywords\n",
        "        self.rank = rank              #rank of own document\n",
        "        self.ngram_overlap = 0        #ngram overlap question\n",
        "        self.proximity = 0            #shortest keywords that cover all keywords\n",
        "        self.score = 0                #Overall score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2qEauYTNg89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def keywords_extraction(sentences):\n",
        "    sent = sentences.lower()\n",
        "    sent = sent.split()\n",
        "    sent = [s for s in sent if s not in stopwords and s not in puct_set]\n",
        "    return sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-zOoQe0NxUq",
        "colab_type": "code",
        "outputId": "2e8146e1-2db9-4e51-9608-1a821d5ff3d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "query = \"Thủ đô của Việt Nam là gì ?\"\n",
        "\n",
        "token_query = tokenize(query)[0]\n",
        "keywords = keywords_extraction(token_query)\n",
        "print(token_query)\n",
        "print('Keywords : ' + ' , '.join(keywords))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thủ_đô của Việt_Nam là gì ?\n",
            "Keywords : thủ_đô , việt_nam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgXmeqVxOvhu",
        "colab_type": "code",
        "outputId": "43b0ce2e-3721-4fc2-eb5c-b4f0688dabf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "start = time.time()\n",
        "service = build(\"customsearch\", \"v1\",developerKey=Seach_api_key)\n",
        "\n",
        "def ggsearch(i):\n",
        "    if (i == 0):\n",
        "        res = service.cse().list(q=query,cx = Custom_Search_Engine_ID).execute()\n",
        "    else:\n",
        "        res = service.cse().list(q=query,cx = Custom_Search_Engine_ID,num=10,start = i*10).execute()\n",
        "    return res['items']\n",
        "\n",
        "#multi processing\n",
        "pool = Pool(4)\n",
        "pages_content = pool.map(ggsearch,range(1))\n",
        "pool.terminate()\n",
        "#pages_content=ggsearch(1)\n",
        "\n",
        "#extract url, title\n",
        "pages_content = [j for i in pages_content for j in i]\n",
        "document_urls = []\n",
        "document_titles = []\n",
        "for page in pages_content:\n",
        "    if 'fileFormat' in page:\n",
        "        print('Skip ' +  page['link'])\n",
        "        continue\n",
        "    document_urls.append(page['link'])\n",
        "    document_titles.append(page[u'title'])\n",
        "    \n",
        "for i in range(0,10):\n",
        "    print(document_titles[i])\n",
        "    print(document_urls[i])\n",
        "\n",
        "print('Number of result: '+str(len(document_titles)))\n",
        "print('Time execute: '+str(time.time() - start))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thủ đô Việt Nam – Wikipedia tiếng Việt\n",
            "https://vi.wikipedia.org/wiki/Th%E1%BB%A7_%C4%91%C3%B4_Vi%E1%BB%87t_Nam\n",
            "Việt Nam Cộng hòa – Wikipedia tiếng Việt\n",
            "https://vi.wikipedia.org/wiki/Vi%E1%BB%87t_Nam_C%E1%BB%99ng_h%C3%B2a\n",
            "Hà Nội – Wikipedia tiếng Việt\n",
            "https://vi.wikipedia.org/wiki/H%C3%A0_N%E1%BB%99i\n",
            "Thảo luận:Thủ đô Việt Nam – Wikipedia tiếng Việt\n",
            "https://vi.wikipedia.org/wiki/Th%E1%BA%A3o_lu%E1%BA%ADn:Th%E1%BB%A7_%C4%91%C3%B4_Vi%E1%BB%87t_Nam\n",
            "Việt Nam – Wikipedia tiếng Việt\n",
            "https://vi.wikipedia.org/wiki/Vi%E1%BB%87t_Nam\n",
            "Thành phố Hồ Chí Minh – Wikipedia tiếng Việt\n",
            "https://vi.wikipedia.org/wiki/Th%C3%A0nh_ph%E1%BB%91_H%E1%BB%93_Ch%C3%AD_Minh\n",
            "Việt Nam Dân chủ Cộng hòa – Wikipedia tiếng Việt\n",
            "https://vi.wikipedia.org/wiki/Vi%E1%BB%87t_Nam_D%C3%A2n_ch%E1%BB%A7_C%E1%BB%99ng_h%C3%B2a\n",
            "Thủ đô – Wikipedia tiếng Việt\n",
            "https://vi.wikipedia.org/wiki/Th%E1%BB%A7_%C4%91%C3%B4\n",
            "Chăm Pa – Wikipedia tiếng Việt\n",
            "https://vi.wikipedia.org/wiki/Ch%C4%83m_Pa\n",
            "Biệt khu Thủ đô – Wikipedia tiếng Việt\n",
            "https://vi.wikipedia.org/wiki/Bi%E1%BB%87t_khu_Th%E1%BB%A7_%C4%91%C3%B4\n",
            "Number of result: 10\n",
            "Time execute: 1.1415157318115234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS1yLfL6UBQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "passages = []\n",
        "total_start = time.time()\n",
        "    \n",
        "#set a limit on number of character extract from url\n",
        "def chunks(text, number):\n",
        "    for i in range(0, len(text), number):\n",
        "        yield text[i:i + number]\n",
        "\n",
        "def getContent(para):\n",
        "    url = para[0]\n",
        "    rank = int((para[1] + 10)/10) - 1 \n",
        "    passages = []\n",
        "    try:\n",
        "        html = requests.get(url, timeout = 5)\n",
        "    except:\n",
        "        print('Cannot read ' + url)\n",
        "        return []\n",
        "    \n",
        "    tree = BeautifulSoup(html.text,'lxml')\n",
        "    for invisible_elem in tree.find_all(['script', 'style']):\n",
        "        invisible_elem.extract()\n",
        "    \n",
        "    sents = []\n",
        "    text_chunks = list(chunks(tree.get_text(),100000))\n",
        "    for text in text_chunks:\n",
        "        sents += tokenize(text)\n",
        "    \n",
        "    for sent in sents:\n",
        "        sent = sent.strip() \n",
        "        if not noiseSent(sent):\n",
        "            sent_keywords = keywords_extraction(sent)\n",
        "            num_overlap_keywords = len(set(sent_keywords) & set(keywords))\n",
        "            if num_overlap_keywords > 0:\n",
        "                passages.append(Passage(sent,rank,num_overlap_keywords))\n",
        "                \n",
        "    return passages\n",
        "    \n",
        "pool = Pool(30)\n",
        "passages = pool.map(getContent,[(document_urls[i],i) for i in range(0,len(document_urls))])\n",
        "pool.terminate()\n",
        "passages = [j for i in passages for j in i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QDirpK1b4tY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "eb7e5e68-d5a6-4918-b7c2-7301d9e61904"
      },
      "source": [
        "for i in range(0,len(passages)):\n",
        "    passages[i].ner = list(set(ner_extraction(passages[i].sent)[\"LOC\"]))\n",
        "\n",
        "print('Number of passages : ' + str(len(passages)))\n",
        "passages = [p for p in passages if len(p.ner) > 0]\n",
        "print('After Filtering : ' + str(len(passages)))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of passages : 948\n",
            "After Filtering : 865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxiXDzzUi4rt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "59c14300-67e8-48c0-90d1-4b355c4a10f6"
      },
      "source": [
        "print( 'Total passages : ' +  str(len(passages)))\n",
        "max_keyword = 0\n",
        "min_num_passages = 20\n",
        "for p in passages:\n",
        "    if p.num_key > max_keyword:\n",
        "        max_keyword = p.num_key\n",
        "        \n",
        "while (True):\n",
        "    num_candidate_passages = 0\n",
        "    for p in passages:\n",
        "        if p.num_key >= max_keyword:\n",
        "            num_candidate_passages += 1\n",
        "    if (num_candidate_passages >= min_num_passages or max_keyword == 1):\n",
        "        break\n",
        "    else:\n",
        "        max_keyword -=1\n",
        "        \n",
        "print( 'Max number of question keyword : ' + str(max_keyword))\n",
        "passages = [p for p in passages if p.num_key >= max_keyword]\n",
        "print( 'After filtering : ' +  str(len(passages)) + '\\n')\n",
        "for i in range(0,min(5,len(passages))):\n",
        "    print( str(i) + ' - ' + passages[i].sent + '\\n')\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total passages : 44\n",
            "Max number of question keyword : 2\n",
            "After filtering : 44\n",
            "\n",
            "0 - Thủ_đô Việt_Nam –_Wikipedia tiếng Việt Thủ_đô Việt_Nam Bách_khoa toàn_thư mở Wikipedia_Buớc tưới chuyển_hướng Bước tới tìm_kiếm Thủ_đô Việt_Nam hiện_nay là thành_phố Hà_Nội .\n",
            "\n",
            "1 - Sau đây là danh_sách các kinh_đô / thủ_đô - hiểu theo nghĩa_rộng - là các trung_tâm chính_trị của chính_thể nhà_nước trong lịch_sử Việt_Nam , và cả của các vương_quốc cổ / cựu quốc_gia từng tồn_tại trên lãnh_thổ Việt_Nam ngày_nay .\n",
            "\n",
            "2 - Mục_lục 1 Kinh_đô và Thủ_đô của Việt_Nam qua các thời_kỳ 2 Kinh_đô không chính_thức 3 Kinh_đô của vương_quốc Chăm Pa cổ 4 Kinh_đô các vương_quốc cổ và cựu quốc_gia khác 5 Thống_kê 6 Ảnh một_số di_tích kinh_đô 7 Cố_đô của Việt_Nam 8 Xem thêm 9 Tham_khảo Kinh_đô và Thủ_đô của Việt_Nam qua các thời_kỳ [ sửa | sửa mã nguồn ]_Phần này liệt_kê các kinh_đô / thủ_đô trong lịch_sử Việt_Nam , theo trình_tự thời_gian .\n",
            "\n",
            "3 - Quốc_hội này ban_hành Hiến_pháp thành_lập Việt_Nam Cộng_hòa trên cơ_sở kế_thừa Quốc_gia Việt_Nam , thủ_đô là thành_phố Sài_Gòn , ngày ban_hành Hiến_pháp 26 tháng 10 trở_thành ngày Quốc_khánh của Đệ_Nhất Cộng_hòa .\n",
            "\n",
            "4 - Trong thời_kỳ chiến_tranh Việt_Nam , Hà_Nội là thủ_đô của Việt_Nam_Dân_chủ_Cộng_hòa và sau khi thống_nhất tiếp_tục là thủ_đô của nước Cộng_hòa_Xã_hội_chủ_nghĩa_Việt_Nam hiện_nay .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}