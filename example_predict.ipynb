{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "example_predict.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/koyomin9zx/UITQA-Vietnamese-Question-Answering/blob/master/example_predict.ipynb",
      "authorship_tag": "ABX9TyP4U+J8CaoQaVQJ9a6yNuY5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koyomin9zx/UITQA-Vietnamese-Question-Answering/blob/master/example_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BslQeHl4qVUj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9a6e1f28-1e80-42d1-f2f5-84cb34a849f0"
      },
      "source": [
        "!git clone https://github.com/koyomin9zx/UITQA-Vietnamese-Question-Answering.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'UITQA-Vietnamese-Question-Answering'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 82 (delta 36), reused 23 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (82/82), done.\n",
            "fatal: destination path 'UITQA-Vietnamese-Question-Answering' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUKLdjyk2kAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch-transformers\n",
        "!pip install underthesea \n",
        "!pip install unidecode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azCO_Yqt5awd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ln -s /content/drive/'My Drive'/data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeOsVwgb65jL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv /content/UITQA-Vietnamese-Question-Answering/combine.py /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X4w72iQ6VhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing libraries \n",
        "from underthesea import word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "from sklearn.metrics.pairwise import cosine_similarity \n",
        "from scipy.spatial import distance \n",
        "from collections import defaultdict, OrderedDict \n",
        "from string import punctuation\n",
        "import unidecode\n",
        "import re\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import glob\n",
        "from combine import QA\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvcP8N1z6bwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(path):\n",
        "  data=[]\n",
        "  all_files = glob.glob(path + \"/*.txt\")\n",
        "  for file in all_files:\n",
        "      passage=open(file, \"r\", encoding='utf-8').read()\n",
        "      data.append(passage)\n",
        "  return data\n",
        "\n",
        "def vi_tokenizer(row):\n",
        "    return word_tokenize(row, format=\"text\")\n",
        "\n",
        "def remove_stopwords(stopwords,hl_split):\n",
        "  sent = [s for s in hl_split if s not in stopwords ]\n",
        "  sent = ' '.join(sent)\n",
        "  return sent\n",
        "\n",
        "def standardize_data(df,stopwords):\n",
        "    hl_cleansed=[]\n",
        "    remove = punctuation\n",
        "    remove = remove.replace(\"_\", \"\")\n",
        "    pattern = \"[{}]\".format(remove) # create the pattern\n",
        "    re_space=re.compile('\\s+')\n",
        "    re_trailing=re.compile('^\\s+|\\s+?$')\n",
        "    for row in df:\n",
        "        row = vi_tokenizer(row)\n",
        "        row=re.sub(pattern, \" \", row) \n",
        "        row=re.sub(re_space,' ',row)\n",
        "        row=re.sub(re_trailing,' ',row)\n",
        "        row = row.strip()\n",
        "        row = remove_stopwords(stopwords,row.split())\n",
        "        #row = remove_accents(row)\n",
        "        row = row.lower()\n",
        "        hl_cleansed.append(row)\n",
        "    return hl_cleansed\n",
        "\n",
        "## Converting 3D array of array into 1D array \n",
        "def arr_convert_1d(arr): \n",
        "    arr = np.array(arr) \n",
        "    arr = np.concatenate( arr, axis=0 ) \n",
        "    arr = np.concatenate( arr, axis=0 ) \n",
        "    return arr \n",
        "   \n",
        "## Cosine Similarity \n",
        "def cosine(trans): \n",
        "    cos = [] \n",
        "    cos.append(cosine_similarity(trans[0], trans[1])) \n",
        "    return cos\n",
        "\n",
        "def tfidf(str1, str2,tf_idf_vetor,stopwords):\n",
        "    str1=standardize_data([str1],stopwords)\n",
        "    str2=standardize_data([str2],stopwords)  \n",
        "    corpus = [str1[0],str2[0]] \n",
        "    trans = tf_idf_vetor.transform(corpus)\n",
        "    cos=cosine(trans) \n",
        "    return arr_convert_1d(cos)[0]\n",
        "\n",
        "def relevant_ranking(query,data,vect,stopwords):\n",
        "  score=defaultdict()\n",
        "  i=0\n",
        "  for d in data:\n",
        "    t=tfidf(query, d,vect,stopwords)\n",
        "    if t!=0.0:\n",
        "      score[t]=d\n",
        "    i+=1\n",
        "  return OrderedDict(sorted(score.items(),reverse=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlQ8dspK2OwP",
        "colab_type": "code",
        "outputId": "c5d438ff-f935-4461-83f0-1eb44e9f5574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "start = time.time()\n",
        "model=QA('/content/data/BERT_Squad_WIki_UIT_pretrain') #path to model\n",
        "end = time.time()\n",
        "print(\"time load model: \"+str(round((end - start),2)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time load model: 34.08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXDa45py6lxK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a9d3ab45-9e7c-4a49-86cc-9a5642614d5d"
      },
      "source": [
        "data=load_data('/content/data/data_QA/data')\n",
        "stopwords = set(open('/content/data/data_QA/stopwords/stopwords.txt').read().split(' ')[:-1])\n",
        "vect = TfidfVectorizer(min_df=1, max_df=0.40,max_features=2000,sublinear_tf=True) \n",
        "stan_data=standardize_data(data,stopwords)\n",
        "vect.fit(stan_data)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
              "        lowercase=True, max_df=0.4, max_features=2000, min_df=1,\n",
              "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
              "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
              "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
              "        vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8B7Io4R5S3R",
        "colab_type": "code",
        "outputId": "804fd867-e5b3-4c65-deb5-94411b8015ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "q=\"Quang Trung mất khi bao nhiêu tuổi ?\"\n",
        "\n",
        "a=relevant_ranking(q,data,vect,stopwords)\n",
        "\n",
        "i=0\n",
        "for score,doc in a.items():\n",
        "  answer = model.predict(doc,q)\n",
        "  print('Question: ',q)\n",
        "  print('\\nAnswer: ',answer['answer'])\n",
        "  print('\\nScore: ',answer['confidence'])\n",
        "  print('\\nContent: ',doc[:200],'...')\n",
        "  print('\\n==========================================\\n\\n\\n')\n",
        "  i+=1\n",
        "  if i==3:\n",
        "    break\n",
        "\n",
        "\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question:  Quang Trung mất khi bao nhiêu tuổi ?\n",
            "\n",
            "Answer:  39\n",
            "\n",
            "Score:  0.4981401622050552\n",
            "\n",
            "Content:  Nguyễn Quang Toản (sinh Qúi Mão 1783- mất Nhâm Tuất 1802)\n",
            "\n",
            "Cảnh Thịnh Hoàng đế (Thời gian ở ngôi 1793-1802)\n",
            "\n",
            "Quang Trung mất ở tuổi 39, khi các con còn nhỏ. Quang Toản là con trưởng mà cũng mới lên 10 ...\n",
            "\n",
            "==========================================\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APLNsClT8xKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punctuation(row):\n",
        "  remove = punctuation\n",
        "  remove = remove.replace(\"_\", \"\")\n",
        "  pattern = \"[{}]\".format(remove) # create the pattern\n",
        "  re_space=re.compile('\\s+')\n",
        "  re_trailing=re.compile('^\\s+|\\s+?$')\n",
        "  row=re.sub(pattern, \" \", row) \n",
        "  row=re.sub(re_space,' ',row)\n",
        "  row=re.sub(re_trailing,' ',row)\n",
        "  row = row.strip()\n",
        "  row =row.lower()\n",
        "  return row"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I8WjRgfDtr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chunks(text, number):\n",
        "    for i in range(0, len(text), number):\n",
        "        yield text[i:i + number]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIFA7vy85yAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentences_tokenize(text):\n",
        "    sents = sent_tokenize(text)\n",
        "    sents = [word_tokenize(s,format = 'text') for s in sents]\n",
        "    sents = [remove_punctuation(s) for s in sents]\n",
        "    #sents = [s.lower() for s in sents]\n",
        "    sents = [remove_stopwords(stopwords,s.split()) for s in sents]\n",
        "    return sents\n",
        "\n",
        "\n",
        "sentences_tokenize('năm sinh của chủ tịch nước Việt Nam là gì ?')\n",
        "\n",
        "\n",
        "data=[sentences_tokenize(i) for i in data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bADBdlqFqvz",
        "colab_type": "code",
        "outputId": "8f05f89b-edb5-43d7-89e9-79064f0d6e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def keywords_extraction(sentences):\n",
        "    sentences=vi_tokenizer(sentences)\n",
        "    sentences=remove_punctuation(sentences)\n",
        "    sentences=remove_stopwords(stopwords,sentences.split())\n",
        "    return sentences\n",
        "\n",
        "keywords_extraction('năm sinh của chủ tịch nước Việt Nam là gì ?')"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'năm sinh chủ_tịch nước việt_nam'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    }
  ]
}