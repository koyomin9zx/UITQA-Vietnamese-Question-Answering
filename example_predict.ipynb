{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "example_predict.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koyomin9zx/UITQA-Vietnamese-Question-Answering/blob/master/example_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BslQeHl4qVUj",
        "colab_type": "code",
        "outputId": "0674d72a-622e-414f-cc0a-9d14ce092a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/koyomin9zx/UITQA-Vietnamese-Question-Answering.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'UITQA-Vietnamese-Question-Answering'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 85 (delta 38), reused 23 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (85/85), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUKLdjyk2kAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch-transformers\n",
        "!pip install underthesea \n",
        "!pip install unidecode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9p5f4LHYj00",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3ad31e30-4c60-4e31-ba19-a37f217749c1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azCO_Yqt5awd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ln -s /content/drive/'My Drive'/data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeOsVwgb65jL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv /content/UITQA-Vietnamese-Question-Answering/combine.py /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X4w72iQ6VhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing libraries \n",
        "from underthesea import word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "from sklearn.metrics.pairwise import cosine_similarity \n",
        "from scipy.spatial import distance \n",
        "from collections import defaultdict, OrderedDict \n",
        "from string import punctuation\n",
        "import unidecode\n",
        "import re\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import glob\n",
        "from combine import QA\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlQ8dspK2OwP",
        "colab_type": "code",
        "outputId": "7d2d5a7c-6128-4da5-8165-cad8416c47d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "start = time.time()\n",
        "model=QA('/content/data/BERT_Squad_WIki_UIT_pretrain') #path to model\n",
        "end = time.time()\n",
        "print(\"time load model: \"+str(round((end - start),2)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time load model: 35.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvcP8N1z6bwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(path):\n",
        "  data=[]\n",
        "  all_files = glob.glob(path + \"/*.txt\")\n",
        "  for file in all_files:\n",
        "      passage=open(file, \"r\", encoding='utf-8').read()\n",
        "      data.append(passage)\n",
        "  return data\n",
        "\n",
        "def vi_tokenizer(row):\n",
        "    return word_tokenize(row, format=\"text\")\n",
        "\n",
        "def remove_stopwords(stopwords,hl_split):\n",
        "  sent = [s for s in hl_split if s not in stopwords ]\n",
        "  sent = ' '.join(sent)\n",
        "  return sent\n",
        "\n",
        "\n",
        "def remove_punctuation(row):\n",
        "  remove = punctuation\n",
        "  remove = remove.replace(\"_\", \"\")\n",
        "  pattern = \"[{}]\".format(remove) # create the pattern\n",
        "  re_space=re.compile('\\s+')\n",
        "  re_trailing=re.compile('^\\s+|\\s+?$')\n",
        "  row=re.sub(pattern, \" \", row) \n",
        "  row=re.sub(re_space,' ',row)\n",
        "  row=re.sub(re_trailing,' ',row)\n",
        "  row = row.strip()\n",
        "  row =row.lower()\n",
        "  return row\n",
        "\n",
        "def standardize_data(df,stopwords):\n",
        "    hl_cleansed=[]\n",
        "    remove = punctuation\n",
        "    remove = remove.replace(\"_\", \"\")\n",
        "    pattern = \"[{}]\".format(remove) # create the pattern\n",
        "    re_space=re.compile('\\s+')\n",
        "    re_trailing=re.compile('^\\s+|\\s+?$')\n",
        "    for row in df:\n",
        "        #row = vi_tokenizer(row)\n",
        "        row=re.sub(pattern, \" \", row) \n",
        "        row=re.sub(re_space,' ',row)\n",
        "        row=re.sub(re_trailing,' ',row)\n",
        "        row = row.strip()\n",
        "        row = remove_stopwords(stopwords,row.split())\n",
        "        #row = remove_accents(row)\n",
        "        row = row.lower()\n",
        "        hl_cleansed.append(row)\n",
        "    return hl_cleansed\n",
        "\n",
        "\n",
        "def sentences_tokenize(text):\n",
        "    sents = sent_tokenize(text)\n",
        "    sents = [word_tokenize(s,format = 'text') for s in sents]\n",
        "    sents = [remove_punctuation(s) for s in sents]\n",
        "    # #sents = [s.lower() for s in sents]\n",
        "    sents = [remove_stopwords(stopwords,s.split()) for s in sents]\n",
        "    return sents\n",
        "\n",
        "\n",
        "## Converting 3D array of array into 1D array \n",
        "def arr_convert_1d(arr): \n",
        "    arr = np.array(arr) \n",
        "    arr = np.concatenate( arr, axis=0 ) \n",
        "    arr = np.concatenate( arr, axis=0 ) \n",
        "    return arr \n",
        "  \n",
        "## Cosine Similarity \n",
        "def cosine(trans): \n",
        "    cos = [] \n",
        "    cos.append(cosine_similarity(trans[0], trans[1])) \n",
        "    return cos\n",
        "\n",
        "def tfidf(str1, str2,tf_idf_vetor,stopwords):\n",
        "    str1=standardize_data([str1],stopwords)\n",
        "    str2=standardize_data([str2],stopwords)  \n",
        "    corpus = [str1[0],str2[0]] \n",
        "    trans = tf_idf_vetor.transform(corpus)\n",
        "    cos=cosine(trans) \n",
        "    return arr_convert_1d(cos)[0]\n",
        "\n",
        "def relevant_ranking(query,data,vect,stopwords):\n",
        "  query=standardize_data([query],stopwords)[0]\n",
        "  print('Query: ',query,'\\n')\n",
        "  score=defaultdict()\n",
        "  i=0\n",
        "  for d in data:\n",
        "    t=tfidf(query, d,vect,stopwords)\n",
        "    if t!=0.0:\n",
        "      score[t]=d\n",
        "    i+=1\n",
        "  return OrderedDict(sorted(score.items(),reverse=True))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXDa45py6lxK",
        "colab_type": "code",
        "outputId": "dcfd062b-f69e-41e7-ccfc-a048dc7dc5f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "data=load_data('/content/data/data_QA/data')\n",
        "stopwords = set(open('/content/data/data_QA/stopwords/stopwords.txt').read().split(' ')[:-1])\n",
        "data_split_sent=[t for i in data for t in sentences_tokenize(i)]\n",
        "vect = TfidfVectorizer(min_df=1, max_df=0.8,max_features=2000,sublinear_tf=True) \n",
        "vect.fit(standardize_data(data,stopwords))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
              "        lowercase=True, max_df=0.8, max_features=2000, min_df=1,\n",
              "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
              "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
              "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
              "        vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8B7Io4R5S3R",
        "colab_type": "code",
        "outputId": "93b6bee5-0a2b-43d2-e528-751bbd7fa493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "q=\"Chu Văn An sinh năm nào ?\"\n",
        "\n",
        "a=relevant_ranking(q,data,vect,stopwords)\n",
        "b=sentences_tokenize(q)[0]\n",
        "i=0\n",
        "for score,doc in a.items():\n",
        "  answer = model.predict(doc,q)\n",
        "  num_overlap=len(set(b.split()) & set(doc.split()))\n",
        "  if answer['confidence']>0.7:\n",
        "    print('\\nQuestion: ',q)\n",
        "    print('\\nOverlap key word: ',num_overlap)\n",
        "    print('\\nAnswer: ',answer['answer'])\n",
        "    print('\\nIR Sccore: ',score)\n",
        "    print('\\nBert Score: ',answer['confidence'])\n",
        "    print('\\nContent: ',doc[:100],'...')\n",
        "    print('\\n==========================================\\n\\n\\n')\n",
        "    i+=1\n",
        "  if i==3:\n",
        "    break\n",
        "\n"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query:  chu văn an sinh năm nào \n",
            "\n",
            "\n",
            "Question:  Chu Văn An sinh năm nào ?\n",
            "\n",
            "Overlap key word:  1\n",
            "\n",
            "Answer:  1292 - 1370\n",
            "\n",
            "IR Sccore:  0.11922763416297272\n",
            "\n",
            "Bert Score:  0.772414214736229\n",
            "\n",
            "Content:  Chu Văn An (1292 - 1370)\n",
            "\n",
            "Chu Văn An - Nhà giáo dục đầu tiên của Việt Nam\n",
            "\n",
            "Chu Văn An (còn gọi là Ch ...\n",
            "\n",
            "==========================================\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Question:  Chu Văn An sinh năm nào ?\n",
            "\n",
            "Overlap key word:  1\n",
            "\n",
            "Answer:  1783\n",
            "\n",
            "IR Sccore:  0.0761225029846985\n",
            "\n",
            "Bert Score:  0.9918439475002891\n",
            "\n",
            "Content:  Nguyễn Quang Toản (sinh Qúi Mão 1783- mất Nhâm Tuất 1802)\n",
            "\n",
            "Cảnh Thịnh Hoàng đế (Thời gian ở ngôi 179 ...\n",
            "\n",
            "==========================================\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Question:  Chu Văn An sinh năm nào ?\n",
            "\n",
            "Overlap key word:  1\n",
            "\n",
            "Answer:  1809\n",
            "\n",
            "IR Sccore:  0.07468928148338269\n",
            "\n",
            "Bert Score:  0.802311580596008\n",
            "\n",
            "Content:  Cao Bá Quát (Kỉ Tị 1809 – Giáp Dần 1854)\n",
            "\n",
            "Danh sĩ thời Tự Đức, tự Chu Thần, hiệu Cúc Đường, biệt hiệ ...\n",
            "\n",
            "==========================================\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bADBdlqFqvz",
        "colab_type": "code",
        "outputId": "8bdd198e-fc9f-48f8-a06e-f665bc69781e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def keywords_extraction(sentences):\n",
        "    sentences=vi_tokenizer(sentences)\n",
        "    sentences=remove_punctuation(sentences)\n",
        "    sentences=remove_stopwords(stopwords,sentences.split())\n",
        "    return sentences.split()\n",
        "\n",
        "keywords_extraction('năm sinh của chủ tịch nước Việt Nam là gì ?')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['năm', 'sinh', 'chủ_tịch', 'nước', 'việt_nam']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    }
  ]
}